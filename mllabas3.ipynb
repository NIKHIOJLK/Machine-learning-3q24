{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dVrWnqQAcRG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "# Step a: Load dataset\n",
        "# Replace with your dataset path if needed\n",
        "df = pd.read_csv('USA_Housing.csv')\n",
        "\n",
        "# Separate input features (X) and target (y)\n",
        "X = df.drop(\"Price\", axis=1).values\n",
        "y = df[\"Price\"].values.reshape(-1, 1)\n",
        "\n",
        "# Step b: Scale the input features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step c: 5-fold cross-validation setup\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_beta = None\n",
        "best_r2 = -np.inf  # initialize with very small value\n",
        "r2_scores = []\n",
        "\n",
        "# Step d: Perform 5-fold CV\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled)):\n",
        "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    # Add bias column of ones for intercept\n",
        "    X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "    X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "    # Compute beta using Least Squares: β = (XᵀX)^(-1)Xᵀy\n",
        "    beta = np.linalg.inv(X_train_bias.T @ X_train_bias) @ X_train_bias.T @ y_train\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = X_test_bias @ beta\n",
        "\n",
        "    # R2 score\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores.append(r2)\n",
        "    print(f\"Fold {fold+1} R2 Score:\", r2)\n",
        "\n",
        "    # Update best beta if this fold is better\n",
        "    if r2 > best_r2:\n",
        "        best_r2 = r2\n",
        "        best_beta = beta\n",
        "\n",
        "print(\"Best R2 from CV:\", best_r2)\n",
        "\n",
        "# Step e: Train-test split (70-30) using best beta\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "y_train_pred = X_train_bias @ best_beta\n",
        "y_test_pred = X_test_bias @ best_beta\n",
        "\n",
        "print(\"Final Train R2:\", r2_score(y_train, y_train_pred))\n",
        "print(\"Final Test R2:\", r2_score(y_test, y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train (56%), Validation (14%), Test (30%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)  # 0.2 of 70% = 14%\n",
        "\n",
        "# Add bias term\n",
        "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
        "X_val_b = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n",
        "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
        "\n",
        "# Gradient Descent Implementation\n",
        "def gradient_descent(X, y, lr, n_iter=1000):\n",
        "    m, n = X.shape\n",
        "    beta = np.zeros((n, 1))\n",
        "    for _ in range(n_iter):\n",
        "        gradients = 2/m * X.T.dot(X.dot(beta) - y)\n",
        "        beta -= lr * gradients\n",
        "    return beta\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    beta = gradient_descent(X_train_b, y_train, lr)\n",
        "\n",
        "    y_val_pred = X_val_b.dot(beta)\n",
        "    y_test_pred = X_test_b.dot(beta)\n",
        "\n",
        "    val_r2 = r2_score(y_val, y_val_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "    results[lr] = {\"beta\": beta, \"val_r2\": val_r2, \"test_r2\": test_r2}\n",
        "\n",
        "# Display best result\n",
        "best_lr = max(results, key=lambda x: results[x][\"val_r2\"])\n",
        "print(\"Best learning rate:\", best_lr)\n",
        "print(\"Validation R2:\", results[best_lr][\"val_r2\"])\n",
        "print(\"Test R2:\", results[best_lr][\"test_r2\"])\n"
      ],
      "metadata": {
        "id": "h_OnK4BxAhva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}