{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704b7386",
   "metadata": {},
   "source": [
    "# Assignment 8 – Q1: SMS Spam Classification using AdaBoost\n",
    "\n",
    "This notebook implements **Q1 (SMS Spam Collection Dataset)** from Assignment 8:\n",
    "\n",
    "1. Data loading and preprocessing (TF–IDF).\n",
    "2. Baseline weak learner (Decision Stump).\n",
    "3. **Manual AdaBoost** implementation (T = 15 rounds).\n",
    "4. **Sklearn AdaBoost** implementation and comparison.\n",
    "\n",
    "> **Note:** Place the dataset file `spam.csv` or `SMSSpamCollection` in the same folder as this notebook before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb78958",
   "metadata": {},
   "source": [
    "## Part A — Data Preprocessing & Exploration\n",
    "\n",
    "Steps:\n",
    "1. Load the SMS spam dataset.\n",
    "2. Convert label: `'spam' → 1`, `'ham' → 0`.\n",
    "3. Text preprocessing: lowercase, remove punctuation, remove stopwords.\n",
    "4. Convert text to numeric features using **TF–IDF**.\n",
    "5. Train–test split (80/20).\n",
    "6. Show class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2105b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load SMS Spam Dataset ===\n",
    "# This cell tries to load a Kaggle-style 'spam.csv'. If your file uses different\n",
    "# column names, adjust accordingly.\n",
    "\n",
    "import os\n",
    "\n",
    "def load_sms_dataset():\n",
    "    # Try common filenames\n",
    "    possible_files = [\n",
    "        'spam.csv',                # Kaggle CSV\n",
    "        'SMSSpamCollection',      # UCI raw text\n",
    "        'SMSSpamCollection.txt'\n",
    "    ]\n",
    "\n",
    "    dataset_path = None\n",
    "    for fname in possible_files:\n",
    "        if os.path.exists(fname):\n",
    "            dataset_path = fname\n",
    "            break\n",
    "\n",
    "    if dataset_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Dataset file not found. Please place 'spam.csv' or 'SMSSpamCollection' in the notebook folder.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Using dataset file: {dataset_path}\\n\")\n",
    "\n",
    "    # Case 1: Kaggle CSV (spam.csv) with columns: v1 (label), v2 (text)\n",
    "    if dataset_path.endswith('.csv'):\n",
    "        df = pd.read_csv(dataset_path, encoding='latin-1')\n",
    "        # Try to standardize column names\n",
    "        if 'label' in df.columns and 'text' in df.columns:\n",
    "            pass\n",
    "        elif 'v1' in df.columns and 'v2' in df.columns:\n",
    "            df = df.rename(columns={'v1': 'label', 'v2': 'text'})\n",
    "        else:\n",
    "            # Keep first two columns as label & text if names are unexpected\n",
    "            df = df.rename(columns={df.columns[0]: 'label', df.columns[1]: 'text'})\n",
    "        # Drop any completely extra unnamed columns\n",
    "        df = df[['label', 'text']]\n",
    "        return df\n",
    "\n",
    "    # Case 2: UCI raw text file 'SMSSpamCollection' (tab-separated)\n",
    "    else:\n",
    "        df = pd.read_csv(dataset_path, sep='\\t', header=None, names=['label', 'text'])\n",
    "        return df\n",
    "\n",
    "\n",
    "df = load_sms_dataset()\n",
    "print('First 5 rows:')\n",
    "display(df.head())\n",
    "print('\\nDataset shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ce071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Encode labels: spam -> 1, ham -> 0 ===\n",
    "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "if df['label_num'].isnull().any():\n",
    "    raise ValueError(\"Found labels other than 'ham' and 'spam'. Check dataset.\")\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "print('\\nNumeric labels distribution:')\n",
    "print(df['label_num'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8154dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Text preprocessing: lowercase, remove punctuation, remove stopwords ===\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation / non-alphanumeric (keep spaces)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    # Tokenize by whitespace\n",
    "    tokens = text.split()\n",
    "    # Remove English stopwords\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].astype(str).apply(preprocess_text)\n",
    "print('Original vs Cleaned (first 5):')\n",
    "display(df[['text', 'clean_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TF–IDF Vectorization ===\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df['clean_text'])\n",
    "y = df['label_num'].values\n",
    "\n",
    "print('Feature matrix shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd83ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train–Test Split (80/20) ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print('Train size:', X_train.shape[0])\n",
    "print('Test size:', X_test.shape[0])\n",
    "\n",
    "# Class distribution in train & test\n",
    "print('\\nClass distribution (train):')\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "print('\\nClass distribution (test):')\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82972f0",
   "metadata": {},
   "source": [
    "## Part B — Weak Learner Baseline (Decision Stump)\n",
    "\n",
    "We train a **Decision Stump**:\n",
    "`DecisionTreeClassifier(max_depth=1)`\n",
    "\n",
    "We will report:\n",
    "- Train accuracy\n",
    "- Test accuracy\n",
    "- Confusion matrix\n",
    "- Brief comment on stump performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d4fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train Decision Stump Baseline ===\n",
    "stump_baseline = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump_baseline.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_stump = stump_baseline.predict(X_train)\n",
    "y_test_pred_stump = stump_baseline.predict(X_test)\n",
    "\n",
    "train_acc_stump = accuracy_score(y_train, y_train_pred_stump)\n",
    "test_acc_stump = accuracy_score(y_test, y_test_pred_stump)\n",
    "\n",
    "print(f'Train Accuracy (Stump): {train_acc_stump:.4f}')\n",
    "print(f'Test Accuracy  (Stump): {test_acc_stump:.4f}\\n')\n",
    "\n",
    "print('Confusion Matrix (Test, Stump):')\n",
    "cm_stump = confusion_matrix(y_test, y_test_pred_stump)\n",
    "print(cm_stump)\n",
    "\n",
    "print('\\nClassification Report (Test, Stump):')\n",
    "print(classification_report(y_test, y_test_pred_stump, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35173e14",
   "metadata": {},
   "source": [
    "### Comment on Stump Performance\n",
    "\n",
    "A single decision stump performs only one split on one feature. For high-dimensional\n",
    "text data (TF–IDF features), the relationship between words and spam/ham labels is\n",
    "complex and cannot be captured by just one split. Therefore, the stump typically has\n",
    "limited accuracy and cannot separate all spam and ham messages effectively.\n",
    "\n",
    "AdaBoost addresses this by **combining many stumps**, each focusing on different\n",
    "mistakes, to form a much stronger classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646eb33",
   "metadata": {},
   "source": [
    "## Part C — Manual AdaBoost (T = 15 rounds)\n",
    "\n",
    "We implement **AdaBoost from scratch** using decision stumps as weak learners.\n",
    "At each iteration we will:\n",
    "\n",
    "- Train a stump with current sample weights.\n",
    "- Compute the weighted error.\n",
    "- Compute the stump weight (alpha).\n",
    "- Print:\n",
    "  - Iteration number\n",
    "  - Misclassified sample indices\n",
    "  - Weights of misclassified samples\n",
    "  - Alpha value\n",
    "- Update and normalize the sample weights.\n",
    "\n",
    "We will also plot:\n",
    "- Iteration vs **weighted error**\n",
    "- Iteration vs **alpha**\n",
    "\n",
    "Finally, we report train/test accuracy and confusion matrix for the **final ensemble**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ce2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Manual AdaBoost Implementation (with decision stumps) ===\n",
    "\n",
    "def manual_adaboost(X_train, y_train, X_test, T=15):\n",
    "    n_samples = X_train.shape[0]\n",
    "    # Initialize weights uniformly\n",
    "    w = np.ones(n_samples) / n_samples\n",
    "\n",
    "    stumps = []\n",
    "    alphas = []\n",
    "    errors = []\n",
    "\n",
    "    for t in range(1, T + 1):\n",
    "        stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "        stump.fit(X_train, y_train, sample_weight=w)\n",
    "        y_pred = stump.predict(X_train)\n",
    "\n",
    "        incorrect = (y_pred != y_train)\n",
    "        eps = np.dot(w, incorrect)  # weighted error (weights sum to 1)\n",
    "\n",
    "        # Avoid division by zero or eps >= 0.5 issues\n",
    "        eps = np.clip(eps, 1e-10, 0.499999)\n",
    "\n",
    "        alpha = 0.5 * np.log((1 - eps) / eps)\n",
    "\n",
    "        # Store\n",
    "        stumps.append(stump)\n",
    "        alphas.append(alpha)\n",
    "        errors.append(eps)\n",
    "\n",
    "        # Print details for this iteration\n",
    "        mis_idx = np.where(incorrect)[0]\n",
    "        print(f\"Iteration {t}\")\n",
    "        print(f\"  Weighted error (eps): {eps:.6f}\")\n",
    "        print(f\"  Alpha: {alpha:.6f}\")\n",
    "        print(f\"  # Misclassified samples: {len(mis_idx)}\")\n",
    "        # WARNING: printing all indices & weights can be very long for large datasets.\n",
    "        # To avoid huge outputs, we show at most first 20 misclassified samples.\n",
    "        max_show = 20\n",
    "        show_idx = mis_idx[:max_show]\n",
    "        print(f\"  Misclassified indices (first {max_show}): {show_idx}\")\n",
    "        print(f\"  Weights of these misclassified samples: {w[show_idx]}\\n\")\n",
    "\n",
    "        # Update weights: increase for misclassified, decrease for correctly classified\n",
    "        # w_i <- w_i * exp(alpha) if misclassified, exp(-alpha) if correct\n",
    "        w[~incorrect] *= np.exp(-alpha)\n",
    "        w[incorrect] *= np.exp(alpha)\n",
    "\n",
    "        # Normalize weights\n",
    "        w /= w.sum()\n",
    "\n",
    "    return stumps, np.array(alphas), np.array(errors)\n",
    "\n",
    "\n",
    "T = 15\n",
    "stumps, alphas, errors = manual_adaboost(X_train, y_train, X_test, T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fbeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Helper: Predict using the manual AdaBoost ensemble ===\n",
    "def adaboost_predict(X, stumps, alphas):\n",
    "    # Aggregate stump predictions with weights (alphas)\n",
    "    stump_preds = np.array([stump.predict(X) for stump in stumps])  # shape: (T, n_samples)\n",
    "    # Convert {0,1} -> {-1, +1}\n",
    "    stump_preds_pm1 = 2 * stump_preds - 1\n",
    "    # Weighted sum\n",
    "    agg = np.dot(alphas, stump_preds_pm1)\n",
    "    # Final prediction: sign -> {0,1}\n",
    "    y_pred_final = (np.sign(agg) == 1).astype(int)\n",
    "    return y_pred_final\n",
    "\n",
    "\n",
    "# Predictions for train and test using manual AdaBoost\n",
    "y_train_pred_ada = adaboost_predict(X_train, stumps, alphas)\n",
    "y_test_pred_ada = adaboost_predict(X_test, stumps, alphas)\n",
    "\n",
    "train_acc_ada = accuracy_score(y_train, y_train_pred_ada)\n",
    "test_acc_ada = accuracy_score(y_test, y_test_pred_ada)\n",
    "\n",
    "print(f\"Manual AdaBoost Train Accuracy: {train_acc_ada:.4f}\")\n",
    "print(f\"Manual AdaBoost Test Accuracy : {test_acc_ada:.4f}\\n\")\n",
    "\n",
    "print('Confusion Matrix (Test, Manual AdaBoost):')\n",
    "cm_ada_manual = confusion_matrix(y_test, y_test_pred_ada)\n",
    "print(cm_ada_manual)\n",
    "\n",
    "print('\\nClassification Report (Test, Manual AdaBoost):')\n",
    "print(classification_report(y_test, y_test_pred_ada, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plots: Iteration vs Weighted Error and Alpha ===\n",
    "iterations = np.arange(1, len(errors) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iterations, errors, marker='o')\n",
    "plt.xlabel('Boosting Round (t)')\n",
    "plt.ylabel('Weighted Error (epsilon_t)')\n",
    "plt.title('Manual AdaBoost: Iteration vs Weighted Error')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(iterations, alphas, marker='o')\n",
    "plt.xlabel('Boosting Round (t)')\n",
    "plt.ylabel('Alpha (alpha_t)')\n",
    "plt.title('Manual AdaBoost: Iteration vs Alpha')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9949208",
   "metadata": {},
   "source": [
    "### Interpretation of Weight Evolution\n",
    "\n",
    "- In early iterations, many samples are misclassified, so the **weighted error** may be\n",
    "  relatively high and the alpha values moderate.\n",
    "- As boosting proceeds, the algorithm **increases weights** on samples that are hard to\n",
    "  classify (often borderline or rare spam/ham messages).\n",
    "- Later stumps focus more on these difficult samples. If the weighted error decreases\n",
    "  over iterations, it indicates that the ensemble is correcting previous mistakes.\n",
    "- Samples that remain misclassified across many rounds accumulate **very high weights**, \n",
    "  showing that AdaBoost is strongly focusing on them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f7f53",
   "metadata": {},
   "source": [
    "## Part D — Sklearn AdaBoost\n",
    "\n",
    "Now we use `sklearn.ensemble.AdaBoostClassifier` with decision stumps as base learners:\n",
    "\n",
    "```python\n",
    "AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.6\n",
    ")\n",
    "```\n",
    "\n",
    "We will:\n",
    "- Train the model\n",
    "- Report train/test accuracy\n",
    "- Show confusion matrix\n",
    "- Compare with the manual AdaBoost implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Sklearn AdaBoost ===\n",
    "base_stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "ada_sklearn = AdaBoostClassifier(\n",
    "    estimator=base_stump,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada_sklearn.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_sklearn = ada_sklearn.predict(X_train)\n",
    "y_test_pred_sklearn = ada_sklearn.predict(X_test)\n",
    "\n",
    "train_acc_sklearn = accuracy_score(y_train, y_train_pred_sklearn)\n",
    "test_acc_sklearn = accuracy_score(y_test, y_test_pred_sklearn)\n",
    "\n",
    "print(f'Sklearn AdaBoost Train Accuracy: {train_acc_sklearn:.4f}')\n",
    "print(f'Sklearn AdaBoost Test Accuracy : {test_acc_sklearn:.4f}\\n')\n",
    "\n",
    "print('Confusion Matrix (Test, Sklearn AdaBoost):')\n",
    "cm_ada_sklearn = confusion_matrix(y_test, y_test_pred_sklearn)\n",
    "print(cm_ada_sklearn)\n",
    "\n",
    "print('\\nClassification Report (Test, Sklearn AdaBoost):')\n",
    "print(classification_report(y_test, y_test_pred_sklearn, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7cf0b",
   "metadata": {},
   "source": [
    "### Comparison: Manual vs Sklearn AdaBoost\n",
    "\n",
    "- Both methods use decision stumps as base learners and combine them via weighted voting.\n",
    "- The **manual implementation** uses a fixed number of boosting rounds `T = 15`, whereas\n",
    "  the sklearn model here uses `n_estimators = 100`, so it may achieve higher accuracy.\n",
    "- If you increase `T` in the manual version, its performance should become closer to\n",
    "  sklearn's implementation.\n",
    "- In practice, sklearn's AdaBoost is optimized and handles edge cases and numerical\n",
    "  stability more robustly, but the manual version is very useful to understand how\n",
    "  weights, errors, and alphas evolve during boosting."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
